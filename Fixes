AUTHOR  : TEJASVAM SINGH

UPDATES


Transitions now conditioned on hidden state length.
Also, Aligner calls method in original align.py to fetch emission paramaters trained by IBM Models.
Trained IBM Model 1 on 10000 sentence pairs for 10 iterations followed by 3 iterations of HMM on 10000 sentences pairs.
AER of .45 was  obtained.



Refactored Likelihood computer into a class. Also, bypassed parameter initializer.
Initialization is now done "on the fly" during Likelihood computation.
Speedup of about 1 minute . 
Currently takes 3 minutes for 1000 sentence pairs for 5 training iterations.


Added The decoder module. Need to test for bugs.


Re-factored into classes.
Fixed training to go over all training instances.
Fixed a serious underflow issue that was causing division by zero errors all over the place.

Resulting algorithm seems to be working fine except I'm not sure that it is fast enough.
I created a small test set of 20 sentences to test and it still takes a few seconds to compute indicating that doing training over the entire set may be prohibitive as of now.
